<div id="header" align="center">
  <img src="./images/coding-cat.gif" alt="Coding Cat" width="200px" height="200px">
</div>

# Hey there! ðŸ‘‹
Hi! I'm Alessandro Dalbesio and I'm currently finishing my master's degree in robotics at Ecole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL) after having completed my bachelor's degree in Automation and Control Engineering at Politecnico di Milano. <br>
I'm passionate about deep learning, computer vision, and robotics and I'm always eager to learn new things!
## Significant projects
Here I will list some of the projects I have worked on or that I am currently working on. <br>
If you want to know more about them, feel free to contact me!

### ![Done Badge](https://img.shields.io/badge/-Completed-blue?style=flat&logo=check&logoColor=white) [Image Analysis and Pattern Recognition](https://github.com/alessandrodalbesio/EPFL-image-analysis-and-pattern-recognition)
The purpose of the project was to develop a pipeline that, given a image of pieces of puzzles randomly placed on a table, was able to detect, segment and cluster the pieces into the different puzzles.  <br><br>
You can find a small presentation, with the steps implemented and the different results, [here](https://github.com/alessandrodalbesio/EPFL-image-analysis-and-pattern-recognition/blob/main/Report.pdf).

### ![Done Badge](https://img.shields.io/badge/-Completed-blue?style=flat&logo=check&logoColor=white) [Basics of Mobile Robotics](https://github.com/alessandrodalbesio/EPFL-basics-of-mobile-robotics)
The purpose of the project was to develop a pipeline such that a robot was able, given it's current position and the destination, to find the best path to reach the destination. <br>
As hardware we used a Thymio robot and an external webcam to detect the position of the robot (such as the current position and the destination but also the obstacles). <br><br>
You can find a visual representation of the results obtained [here](https://github.com/alessandrodalbesio/EPFL-basics-of-mobile-robotics/blob/main/README.md).

### ![Done Badge](https://img.shields.io/badge/-Completed-blue?style=flat&logo=check&logoColor=white) [Rocket control with MPC techniques](https://github.com/alessandrodalbesio/EPFL-model-predictive-control)
The purpose of the project was to develop different model predictive controllers to control the trajectory of a rocket. 
 The following controllers have been implemented:
 - Linear MPC on a linear rocket model
 - Linear MPC on a non-linear rocket model
 - Non linear MPC on a non-linear rocket model 

 Additionally the rocket has been studied both with fuel consumption (to simulate an external disturbance) and delay. <br><br>
 You can find the report for this project [here](https://github.com/alessandrodalbesio/EPFL-model-predictive-control/blob/master/report.pdf).

### ![Done Badge](https://img.shields.io/badge/-Completed-blue?style=flat&logo=check&logoColor=white) [Deep learning for autonomous vehicles](https://github.com/alessandrodalbesio/DLAV-project)
In this project we had to work with the [UniTraj framework](https://github.com/vita-epfl/UniTraj) to control a car in a simulated environment. <br>
The final goal was to implement the [QCNet model](https://github.com/ZikangZhou/QCNet) into the UniTraj framework and test it in the simulated environment. <br><br>
For this project no report was written, but you can find the code [here](https://github.com/alessandrodalbesio/DLAV-project).
### ![Done Badge](https://img.shields.io/badge/-Completed-blue?style=flat&logo=check&logoColor=white) Mixed Reality Harvesting Project
In this project I created a mixed reality environment in which a user could interact with a real object and a virtual object. <br><br>
This project required the integration of lots of different components:
- Raspberry Pi 3B+: on this device the following components were running:
  - An [API server](https://github.com/alessandrodalbesio/MRE-harvesting-API-server): this interface handle the upload of the objects, colors and other information needed either by the user interface or the virtual environment.
  - An [User Interface](https://github.com/alessandrodalbesio/MRE-harvesting-user-interface): this interface was used to let the user choose the object to interact with and manage the virtual environment.
  - A [Websocket server](https://github.com/alessandrodalbesio/MRE-harvesting-websocket-server): used to handle real time synchronization between the device (to avoid pooling).
- OptiTrack: this system was used to track the position of the real object
  - [Adapted OptiTrack Python SDK](https://github.com/alessandrodalbesio/MRE-harvesting-MRE-optitrack-interface): Modified version of the OptiTrack Python SDK to handle socket connection.
- Oculus Quest 2: the user was wearing this device to see the virtual environment
  - [Virtual Environment (Unity)](https://github.com/alessandrodalbesio/MRE-harvesting-virtual-environment): implementation of the virtual environment in Unity.
## Tools and languages
I'm passionate about programming and during the years I've tried different languages and tools. Here are some of the ones I'm most familiar with:
- <b>General purpose programming languages</b> (Python, C, Java)
- <b>Front-end development</b> (HTML, CSS, JavaScript)
- <b>Back-end development</b> (PHP, SQL)
- <b>App development</b> (Flutter, React Native)
- <b>Cloud computing</b> (Google Cloud, Firebase)
- <b>General purpose</b> (Git, Docker, Linux)

Regarding computer vision and deep learning, I have experience with:
- <b>Deep learning frameworks</b> (PyTorch, TensorFlow (Basics))
- <b>Computer vision libraries</b> (OpenCV, PIL)
- <b>Data manipulation libraries</b> (Pandas, NumPy)

I love always to learn new things and I've no problem in learning new languages or tools if needed for a project.

## Contact me
Soon available!